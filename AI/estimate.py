import re
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain

def run_hybrid_rag_query(vectordb, query):
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(query)

    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template="""
ë‹¹ì‹ ì€ ê³µê³µì‹œì„¤ë¬¼ì˜ ì² ê±°Â·ìˆ˜ë¦¬ ë“±ì˜ ê²¬ì ì„ ê³„ì‚°í•˜ëŠ” ì „ë¬¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ê²€ìƒ‰ëœ ì°¸ê³  ë¬¸ì„œì…ë‹ˆë‹¤. ì´ ë¬¸ì„œë“¤ì—ëŠ” ê°€ê²© ì‚°ì •ì˜ ê·¼ê±°ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1. ë¬¸ì„œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•´ ì˜ˆìƒ ê²¬ì ì„ 'ì› ë‹¨ìœ„'ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
2. ë¬¸ì„œì— ì§ì ‘ì  ê·¼ê±°ê°€ ë¶€ì¡±í•  ê²½ìš°, ìì‹ ì˜ ì§€ì‹ìœ¼ë¡œ ì¶”ì •í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ë‹¨ê°€ì™€ ìì¬ë¹„, ì¸ê±´ë¹„ ë“±ì„ ê°€ì •í•˜ì—¬ ìµœëŒ€í•œ ì„¤ë“ë ¥ ìˆëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
3. 'ì •í™•í•˜ì§€ ì•Šë‹¤', 'ëª¨ë¥´ê² ë‹¤'ëŠ” í‘œí˜„ì€ í”¼í•˜ê³ , ëŒ€ëµì ì¸ ìˆ˜ì¹˜ë¼ë„ ë°˜ë“œì‹œ ì¶”ì •í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.
4. ê³„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”:

ì˜ˆìƒ ê²¬ì  (ì›): [ìˆ«ì]
ğŸ“Œ ê³„ì‚° ê·¼ê±° ìš”ì•½: [ë‹¨ê°€, ì¸ê±´ë¹„, ì¥ë¹„ë¹„, ìˆ˜ëŸ‰ ë“±ì˜ ê°€ì • ì„¤ëª…]
ğŸ“š ì°¸ê³  ë¬¸ì„œ ë‚´ìš© ìš”ì•½: [ê²€ìƒ‰ ë˜ëŠ” ì°¸ì¡°ëœ ë¬¸ì„œ ìš”ì•½]

í•´ë‹¹ ì‹œì„¤ë¬¼ì˜ ì² ê±° ë˜ëŠ” ìˆ˜ë¦¬ ê²¬ì ì´ ë¬¸ì„œì—ì„œ ì§ì ‘ ì œê³µë˜ì§€ ì•Šë”ë¼ë„, ë°˜ë“œì‹œ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ê·¼ì‚¬ ì¶”ì •ì„ ì‹œë„í•´ ì£¼ì„¸ìš”:

- ì¼ë°˜ì ì¸ ê³µê³µì‹œì„¤ë¬¼ ë‹¨ê°€
- ìì¬ ë° ì¥ë¹„ ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜
- ì¸ê±´ë¹„ ê¸°ì¤€
- í•œêµ­ì˜ ì§€ë°©ìì¹˜ë‹¨ì²´ ë°œì£¼ ê³µì‚¬ ì‚¬ë¡€

ì§ì ‘ ì •ë³´ê°€ ì—†ì–´ë„, LLMì´ ë³´ìœ í•œ í†µê³„ë‚˜ ë‹¨ê°€ ê¸°ì¤€, ë…¼ë¦¬ì  ì¶”ì •ì— ê¸°ë°˜í•˜ì—¬ **ì› ë‹¨ìœ„ ê²¬ì **ì„ ë°˜ë“œì‹œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ë‹µë³€:
""".strip()
    )

    llm = ChatOpenAI(model_name="gpt-5")
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    stuff_chain = StuffDocumentsChain(
        llm_chain=llm_chain,
        document_variable_name="context"
    )

    raw_answer = stuff_chain.invoke({
        "input_documents": docs,
        "question": query
    })

    if isinstance(raw_answer, dict) and "output_text" in raw_answer:
        output_text = raw_answer["output_text"]
    elif isinstance(raw_answer, str):
        output_text = raw_answer
    else:
        output_text = str(raw_answer)

    # ê²¬ì  ì¶”ì¶œ
    estimate_match = re.search(r"ì˜ˆìƒ\s*ê²¬ì \s*\(ì›\)\s*[:ï¼š]\s*([\d,]+)", output_text)
    try:
        estimate_val = int(estimate_match.group(1).replace(",", "")) if estimate_match else None
    except:
        estimate_val = None

    # ê³„ì‚° ê·¼ê±° ìš”ì•½
    basis_match = re.search(r"ğŸ“Œ\s*ê³„ì‚°\s*ê·¼ê±°\s*ìš”ì•½\s*[:ï¼š]\s*(.+?)(?=\nğŸ“š|\Z)", output_text, re.S)
    basis_text = basis_match.group(1).strip() if basis_match else ""

    # ì°¸ê³  ë¬¸ì„œ ë‚´ìš© ìš”ì•½
    ref_match = re.search(r"ğŸ“š\s*ì°¸ê³ \s*ë¬¸ì„œ\s*ë‚´ìš©\s*ìš”ì•½\s*[:ï¼š]\s*(.+)", output_text, re.S)
    ref_text = ref_match.group(1).strip() if ref_match else ""

    # ğŸ“Œ ë¼ë²¨ í¬í•¨í•´ì„œ í•©ì¹˜ê¸°
    if basis_text or ref_text:
        estimate_basis = ""
        if basis_text:
            estimate_basis += "ğŸ“Œ ê³„ì‚° ê·¼ê±° ìš”ì•½:\n" + basis_text
        if ref_text:
            if estimate_basis:
                estimate_basis += "\n\n"
            estimate_basis += "ğŸ“š ì°¸ê³  ë¬¸ì„œ ë‚´ìš© ìš”ì•½:\n" + ref_text
    else:
        estimate_basis = ""

    meta_docs = [
        {
            "id": getattr(doc, "id", None),
            "metadata": doc.metadata
        }
        for doc in docs
    ]

    return {
        "estimate": estimate_val,
        "estimate_basis": estimate_basis,
        "raw_answer": output_text
    }, meta_docs
