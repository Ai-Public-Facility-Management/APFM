import json
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from .schema import parser

def generate_proposal(
    estimations: list
    # í•„ìš”í•˜ë‹¤ë©´ ë‹¤ë¥¸ í•­ëª©ë“¤(project_overview ë“±)ë„ ì¶”ê°€ ê°€ëŠ¥
) -> dict:
    estimations_json = json.dumps(estimations, ensure_ascii=False, indent=2)

    document_format = """
    [ê³µ ì‚¬ ëª…] {project_name}
    [ê³µì‚¬ê°œìš”] {project_overview}
    [ê³µì‚¬ ì†Œìš” ê¸°ê°„] {construction_period}
    [í˜„ì¥ ë¶„ì„ ìš”ì•½] {site_analysis_summary}
    [ì„¸ë¶€ ê²¬ì  ë° ê³„ì‚° ê·¼ê±° ìš”ì•½] {estimation_details_with_basis}
    [ì´ ê¸ˆì•¡] {total_cost}
    [ì‘ì—… ì¸ë ¥ ê³„íš] {manpower_plan}
    [ì¥ë¹„ ê³„íš] {equipment_plan}
    [ì•ˆì „ ë° í’ˆì§ˆê´€ë¦¬ ë°©ì•ˆ] {safety_quality_plan}
    [ê²°ë¡  ë° ê¸°ëŒ€íš¨ê³¼] {conclusion_expected_effect}
    """

    prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ê±´ì„¤ ì…ì°° ì œì•ˆì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ 'ë¬¸ì„œ í˜•ì‹'ê³¼ 'ê²¬ì  ë°ì´í„°'ë¥¼ ì°¸ê³ í•˜ì—¬ ì œì•ˆì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ë©°, í‚¤ ì´ë¦„ì€ {format_instructions}ë¥¼ ë”°ë¥´ì„¸ìš”.

    [ë¬¸ì„œ í˜•ì‹]
    {doc_format}

    [ì‘ì„± ê·œì¹™]
    - ê° í•­ëª©ì˜ ë‚´ìš©ì€ í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œì²´ë¡œ ì‘ì„±
    - ê¸ˆì•¡ì€ 3ìë¦¬ ì½¤ë§ˆë¡œ í‘œê¸°
    - 'estimation_details_with_basis'ëŠ” ê° ê²¬ì ì˜ ì‘ì—…ëª…ê³¼ ğŸ“Œ ê³„ì‚° ê·¼ê±° ìš”ì•½ë§Œ í¬í•¨
    - ê³„ì‚° ê·¼ê±°ëŠ” ì¤„ë°”ê¿ˆ(\n)ìœ¼ë¡œ êµ¬ë¶„ëœ bullet('- ') í˜•ì‹
    - 'construction_period'ëŠ” ì‘ì—… ë‚œì´ë„, ë©´ì , ì¸ë ¥ íˆ¬ì…ëŸ‰ ê³ ë ¤ ì˜ˆìƒ ì¼ìˆ˜ ì‘ì„±

    [ê²¬ì  ë°ì´í„°]
    {estimations}
    """)

    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.3)
    prompt_with_schema = prompt.partial(
        format_instructions=parser.get_format_instructions(),
        doc_format=document_format
    )

    filled_proposal = (prompt_with_schema | llm | parser).invoke({"estimations": estimations_json})

    return filled_proposal
